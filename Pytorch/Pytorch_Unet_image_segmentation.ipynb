{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1df0368-e217-47af-9d8c-4f616b689182",
   "metadata": {},
   "source": [
    "# Pytorch Unet image segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e54e6c95-0956-46a2-94d8-b09fbd128d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04b7eabc-1ae3-49c1-9562-f03faf39ff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa35436-3d39-4bc0-bdf2-65649a0957da",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90fa262a-fa75-41fa-b805-b4fac80ce475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the indicator of a fresh run\n",
    "first_time_running = False\n",
    "\n",
    "# user-specified working directory\n",
    "filepath = '/glade/scratch/ksha/DATA/oxford_iiit/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fdff38e-ab8d-4ed0-96b3-348f27d74f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if first_time_running:\n",
    "    # downloading and executing data files\n",
    "    import tarfile\n",
    "    import urllib.request\n",
    "    \n",
    "    filename_image = filepath+'images.tar.gz'\n",
    "    filename_target = filepath+'annotations.tar.gz'\n",
    "    \n",
    "    urllib.request.urlretrieve('http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz', filename_image);\n",
    "    urllib.request.urlretrieve('https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz', filename_target);\n",
    "    \n",
    "    with tarfile.open(filename_image, \"r:gz\") as tar_io:\n",
    "        tar_io.extractall(path=filepath)\n",
    "    with tarfile.open(filename_target, \"r:gz\") as tar_io:\n",
    "        tar_io.extractall(path=filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f25e57-1944-4f51-9542-19438d35b5ff",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ddd534-68fb-4c51-916d-25d28e68b22e",
   "metadata": {},
   "source": [
    "https://apple.github.io/coremltools/docs-guides/source/flexible-inputs.html#set-the-range-for-each-dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e329746b-d158-468e-b2a7-a47360766e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/ksha/conda-envs/tf212gpu/lib/python3.10/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "/glade/work/ksha/conda-envs/tf212gpu/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /glade/u/home/ksha/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 398MB/s]\n"
     ]
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(self, in_channels, middle_channels, out_channels):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "    self.conv_relu = nn.Sequential(\n",
    "        nn.Conv2d(middle_channels, out_channels, kernel_size=3, padding=1),\n",
    "        nn.ReLU(inplace=True))\n",
    "      \n",
    "  def forward(self, x1, x2):\n",
    "    x1 = self.up(x1)\n",
    "    x1 = torch.cat((x1, x2), dim=1)\n",
    "    x1 = self.conv_relu(x1)\n",
    "    return x1\n",
    "\n",
    "class unet_maker(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_model = torchvision.models.resnet18(True)\n",
    "        self.base_layers = list(self.base_model.children())\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False),\n",
    "            self.base_layers[1],\n",
    "            self.base_layers[2])\n",
    "        self.layer2 = nn.Sequential(*self.base_layers[3:5])\n",
    "        self.layer3 = self.base_layers[5]\n",
    "        self.layer4 = self.base_layers[6]\n",
    "        self.layer5 = self.base_layers[7]\n",
    "        self.decode4 = Decoder(512, 256+256, 256)\n",
    "        self.decode3 = Decoder(256, 256+128, 256)\n",
    "        self.decode2 = Decoder(256, 128+64, 128)\n",
    "        self.decode1 = Decoder(128, 64+64, 64)\n",
    "        self.decode0 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1, bias=False),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False)\n",
    "            )\n",
    "        self.conv_last = nn.Conv2d(64, n_class, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        e1 = self.layer1(input) # 64,128,128\n",
    "        e2 = self.layer2(e1) # 64,64,64\n",
    "        e3 = self.layer3(e2) # 128,32,32\n",
    "        e4 = self.layer4(e3) # 256,16,16\n",
    "        f = self.layer5(e4) # 512,8,8\n",
    "        d4 = self.decode4(f, e4) # 256,16,16\n",
    "        d3 = self.decode3(d4, e3) # 256,32,32\n",
    "        d2 = self.decode2(d3, e2) # 128,64,64\n",
    "        d1 = self.decode1(d2, e1) # 64,128,128\n",
    "        d0 = self.decode0(d1) # 64,256,256\n",
    "        out = self.conv_last(d0) # 1,256,256\n",
    "        return out\n",
    "        \n",
    "model = unet_maker(n_class=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "efd0dbd3-b6d9-44ff-a32f-966e44fe7cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cf18fa-cade-40de-8347-4267795dad5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da422666-611a-4092-98b1-ad42c9c13e64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1364c65-9bce-4ac7-b626-517e5bff548d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def input_data_process(input_array):\n",
    "    '''converting pixel vales to [0, 1]'''\n",
    "    return input_array/255.\n",
    "\n",
    "def shuffle_ind(L):\n",
    "    ind = np.arange(L)\n",
    "    np.random.shuffle(ind)\n",
    "    return ind\n",
    "\n",
    "def image_to_array(filenames, size, channel):\n",
    "    # number of files\n",
    "    L = len(filenames)\n",
    "    \n",
    "    # allocation\n",
    "    out = np.empty((L, size, size, channel))\n",
    "    \n",
    "    # loop over filenames\n",
    "    if channel == 1:\n",
    "        for i, name in enumerate(filenames):\n",
    "            with Image.open(name) as pixio:\n",
    "                pix = pixio.resize((size, size), Image.NEAREST)\n",
    "                out[i, ..., 0] = np.array(pix)\n",
    "    else:\n",
    "        for i, name in enumerate(filenames):\n",
    "            with Image.open(name) as pixio:\n",
    "                pix = pixio.resize((size, size), Image.NEAREST)\n",
    "                out[i, ...] = np.array(pix)[..., :channel]\n",
    "    return out[:, ::-1, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "73eaeeeb-1245-442e-ab10-603b99d60342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:validation:testing = 5912:739:739\n"
     ]
    }
   ],
   "source": [
    "sample_names = np.array(sorted(glob(filepath+'images/*.jpg')))\n",
    "label_names = np.array(sorted(glob(filepath+'annotations/trimaps/*.png')))\n",
    "\n",
    "L = len(sample_names)\n",
    "ind_all = shuffle_ind(L)\n",
    "\n",
    "L_train = int(0.8*L); L_valid = int(0.1*L); L_test = L - L_train - L_valid\n",
    "ind_train = ind_all[:L_train]; ind_valid = ind_all[L_train:L_train+L_valid]; ind_test = ind_all[L_train+L_valid:]\n",
    "print(\"Training:validation:testing = {}:{}:{}\".format(L_train, L_valid, L_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "341e6e4a-3ac0-4891-9c43-30933557c665",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_input = input_data_process(image_to_array(sample_names[ind_valid], size=256, channel=3))\n",
    "valid_target = image_to_array(label_names[ind_valid], size=256, channel=1)\n",
    "\n",
    "valid_input = np.transpose(valid_input, [0, 3, 1, 2])\n",
    "valid_input = torch.from_numpy(valid_input).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "56a52d2c-2aad-43d9-a5f0-767f8ad9215e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = input_data_process(image_to_array(sample_names[ind_test], size=256, channel=3))\n",
    "test_target = image_to_array(label_names[ind_test], size=256, channel=1)\n",
    "\n",
    "test_input = np.transpose(test_input, [0, 3, 1, 2])\n",
    "test_input = torch.from_numpy(test_input).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "158b227c-53fe-46df-b281-282235d85fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "03441dd6-9396-49e3-9f22-c40fc91ee8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/glade/work/ksha/torch_models/oxford_unet.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7f00d999-b113-48da-b229-1d4c73ed1854",
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputs = model(valid_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4868c994-5fbe-48c7-8299-be3327675a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6774ea2-38dc-4390-9a69-7a46ec57a296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c0995d-8039-427d-b353-0912b5d0af22",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_del = 0.0\n",
    "max_tol = 3 # early stopping with 2-epoch patience\n",
    "tol = 0\n",
    "\n",
    "y_pred = model(input_test)\n",
    "record = loss_func(y_pred, y_true).detach().numpy()\n",
    "print('Initial loss: {}'.format(record))\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(trainloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # on-epoch-end validation\n",
    "    y_pred = model(input_test)\n",
    "    record_temp = loss_func(y_pred, y_true).detach().numpy()\n",
    "    print('Validation loss: {}'.format(record_temp))\n",
    "    \n",
    "    if record - record_temp > min_del:\n",
    "        print('Validation loss improved from {} to {}'.format(record, record_temp))\n",
    "        record = record_temp\n",
    "        print(\"Save to {}\".format(save_dir))\n",
    "        torch.save(model.state_dict(), save_dir)\n",
    "        \n",
    "    else:\n",
    "        print('Validation loss {} NOT improved'.format(record_temp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
